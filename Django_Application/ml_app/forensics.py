import cv2
import numpy as np
import logging
logger = logging.getLogger(__name__)
# embedding helper

def bg_face_flow_ratio(prev_frame, curr_frame, face_box, ring=15):
    """
    Compute the ratio of optical flow magnitude in the background ring vs. inside the face box.
    High ratio suggests FOMM-like warping artifacts.
    """
    x1, y1, x2, y2 = face_box
    prev_g = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    curr_g = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)
    flow = cv2.calcOpticalFlowFarneback(prev_g, curr_g, None, 0.5, 3, 15, 3, 5, 1.2, 0)
    mag = np.linalg.norm(flow, axis=2)

    face_mag = mag[y1:y2, x1:x2]
    if face_mag.size == 0:
        return 0.0

    pad = ring
    y1o, y2o = max(0, y1 - pad), min(mag.shape[0], y2 + pad)
    x1o, x2o = max(0, x1 - pad), min(mag.shape[1], x2 + pad)
    ring_mag = mag[y1o:y2o, x1o:x2o].copy()
    ring_mag[y1:y2, x1:x2] = np.nan

    face_val = np.median(face_mag)
    ring_val = np.nanmedian(ring_mag) if np.isfinite(np.nanmedian(ring_mag)) else 0.0
    return float(ring_val) / (face_val + 1e-6)

def boundary_flicker(prev_frame, curr_frame, face_box, ring=6):
    """
    Compute temporal gradient difference around the boundary of the detected face box.
    High flicker indicates FOMM occlusion/warping seams.
    """
    x1, y1, x2, y2 = face_box
    prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
    curr = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)

    gx_prev = cv2.Sobel(prev, cv2.CV_32F, 1, 0, ksize=3)
    gy_prev = cv2.Sobel(prev, cv2.CV_32F, 0, 1, ksize=3)
    gx_curr = cv2.Sobel(curr, cv2.CV_32F, 1, 0, ksize=3)
    gy_curr = cv2.Sobel(curr, cv2.CV_32F, 0, 1, ksize=3)

    mask = np.zeros(prev.shape, np.uint8)
    cv2.rectangle(mask, (x1 - ring, y1 - ring), (x2 + ring, y2 + ring), 1, thickness=ring * 2)
    cv2.rectangle(mask, (x1, y1), (x2, y2), 0, thickness=-1)
    mask = mask.astype(bool)

    prev_grad = np.hypot(gx_prev, gy_prev)[mask]
    curr_grad = np.hypot(gx_curr, gy_curr)[mask]
    if prev_grad.size == 0 or curr_grad.size == 0:
        return 0.0
    return float(np.std(curr_grad - prev_grad))

def spectral_slope(face_gray, eps=1e-6):
    """
    Estimate spectral slope from FFT of a grayscale face crop.
    Warping often flattens the slope (less negative).
    """
    try:
        f = np.fft.fftshift(np.fft.fft2(face_gray.astype(np.float32)))
        p = np.abs(f) ** 2
        h, w = p.shape
        cy, cx = h // 2, w // 2
        yy, xx = np.ogrid[:h, :w]
        r = np.sqrt((yy - cy) ** 2 + (xx - cx) ** 2).astype(np.int32)
        rmax = r.max()
        radial = np.bincount(r.ravel(), p.ravel()) / (np.bincount(r.ravel()) + eps)
        lo, hi = int(0.15 * rmax), int(0.6 * rmax)
        if hi <= lo or hi > len(radial):
            return -2.0
        x = np.log(np.arange(lo, hi) + 1)
        y = np.log(radial[lo:hi] + eps)
        a, _ = np.polyfit(x, y, 1)
        return float(a)
    except Exception as e:
        logger.warning(f"spectral_slope failed: {e}")
        return -2.0

def identity_variance(embeddings):
    """
    Compute identity variance from a list of face embeddings (list of 1D numpy arrays).
    Returns a score in [0,100] where higher means more identity inconsistency (suspicious).
    """
    if not embeddings or len(embeddings) < 2:
        return 0.0
    X = np.stack(embeddings)
    # L2-normalize
    norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-8
    Xn = X / norms
    # pairwise cosine distances to the median embedding
    med = np.median(Xn, axis=0)
    med = med / (np.linalg.norm(med) + 1e-8)
    sims = Xn.dot(med)
    dists = 1.0 - sims  # cosine distance
    # aggregate robustly
    med_dist = float(np.median(dists))
    # map to 0-100 using a gentle sigmoid: suspicious when median distance > ~0.15
    score = 100.0 * (1.0 / (1.0 + np.exp(-20.0 * (med_dist - 0.15))))
    return float(np.clip(score, 0.0, 100.0))

def fomm_likelihood(scores):
    """
    Aggregate forensic signals into a FOMM likelihood score (0-100).
    Higher means more likely to be generated by FOMM.
    Uses sigmoid-based adaptive scaling to avoid hard-coded thresholds.
    """
    flow = np.median(scores["bg_flow_ratio"]) if scores["bg_flow_ratio"] else 0.0
    flick = np.percentile(scores["boundary_flicker"], 75) if scores["boundary_flicker"] else 0.0
    spec = np.median(scores["spec_slope"]) if scores["spec_slope"] else -2.0

    logger.info(f"FOMM raw scores: flow={flow:.4f}, flick={flick:.4f}, spec={spec:.4f}")

    # Sigmoid-based adaptive transforms (steepness and centers tuned conservatively)
    # flow: suspicious when > ~0.2
    flow_s = 1.0 / (1.0 + np.exp(-12.0 * (flow - 0.20)))
    # flick: suspicious when > ~2.0
    flick_s = 1.0 / (1.0 + np.exp(-0.6 * (flick - 2.0)))
    # spec: suspicious when less negative than ~-1.8 (i.e. spec gets larger)
    spec_s = 1.0 / (1.0 + np.exp(6.0 * (spec + 1.8)))

    # Weighted combination (keep conservative influence)
    score = 100.0 * (0.35 * flow_s + 0.35 * flick_s + 0.30 * spec_s)
    if not np.isfinite(score):
        score = 0.0
    return float(np.clip(score, 0.0, 100.0))